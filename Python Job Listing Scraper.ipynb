{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJfriOY-4ehh",
        "outputId": "6dee781d-c9d7-400a-ee15-ec8ba87aca71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to website...\n",
            "Found 100 job listings. Starting extraction...\n",
            "Success! Data saved to 'job_results.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "OUTPUT_FILE = 'job_results.csv'\n",
        "\n",
        "def scrape_jobs():\n",
        "    \"\"\"\n",
        "    Scrapes job listings from the fake-jobs website and saves them to a CSV file.\n",
        "    \"\"\"\n",
        "    print(\"Connecting to website...\")\n",
        "    # 1. Get the page content\n",
        "    page = requests.get(URL)\n",
        "\n",
        "    # Check if connection was successful\n",
        "    if page.status_code != 200:\n",
        "        print(f\"Failed to load page. Status code: {page.status_code}\")\n",
        "        return\n",
        "\n",
        "    # 2. Parse HTML content\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    # 3. Find all job cards\n",
        "    job_cards = soup.find_all(\"div\", class_=\"card-content\")\n",
        "    print(f\"Found {len(job_cards)} job listings. Starting extraction...\")\n",
        "\n",
        "    # 4. Prepare CSV file\n",
        "    try:\n",
        "        with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "\n",
        "            # Write Header\n",
        "            writer.writerow(['Job Title', 'Company', 'Location', 'Apply Link'])\n",
        "\n",
        "            # 5. Loop through each card\n",
        "            for card in job_cards:\n",
        "\n",
        "                # --- EXTRACT DATA & HANDLE EDGE CASES ---\n",
        "\n",
        "                # Job Title\n",
        "                title_elem = card.find(\"h2\", class_=\"title\")\n",
        "                title = title_elem.text.strip() if title_elem else \"Title Missing\"\n",
        "\n",
        "                # Company Name\n",
        "                company_elem = card.find(\"h3\", class_=\"company\")\n",
        "                company = company_elem.text.strip() if company_elem else \"Company Missing\"\n",
        "\n",
        "                # Location\n",
        "                location_elem = card.find(\"p\", class_=\"location\")\n",
        "                location = location_elem.text.strip() if location_elem else \"Location Missing\"\n",
        "\n",
        "                # Apply Link\n",
        "                links = card.find_all(\"a\")\n",
        "                if len(links) >= 2:\n",
        "                    apply_url = links[1][\"href\"]\n",
        "                else:\n",
        "                    apply_url = \"Link Missing\"\n",
        "\n",
        "                # --- SAVE ROW ---\n",
        "                writer.writerow([title, company, location, apply_url])\n",
        "\n",
        "        print(f\"Success! Data saved to '{OUTPUT_FILE}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while writing the file: {e}\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_jobs()"
      ]
    }
  ]
}